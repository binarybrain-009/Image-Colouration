<!DOCTYPE html>
<html lang="en"><head>  
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Computer Vision Course Project
  | ECE, Virginia Tech | Fall 2021: ECE 4554/5554</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">

<!-- Le styles -->  
<link href="css/bootstrap.css" rel="stylesheet">
<style>

.custom-img {
    width: 3000; /* or set a specific value like 500px */
    height: 3000; /* Maintains the aspect ratio */
    object-fit: contain; /* Ensures the image fits inside the container without distortion */
}

body {
padding-top: 60px; /* 60px to make the container go all the way to the bottom of the topbar */
}
.vis {
color: #3366CC;
}
.data {
color: #FF9900;
}
</style>
  
<link href="css/bootstrap-responsive.min.css" rel="stylesheet">

<!-- HTML5 shim, for IE6-8 support of HTML5 elements --><!--[if lt IE 9]>
<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->
</head>

<body>
<div class="container">
<div class="page-header">

<!-- Title and Name --> 
<h1>Gray to Glory: Performance Analysis of Image Coloration Models</h1> 
<span style="font-size: 20px; line-height: 1.5em;"><strong>Gauransh Sawhney, Daksh Dave, Tushar Deshpande</strong></span><br>
<span style="font-size: 18px; line-height: 1.5em;">Fall 2024 ECE 4554/5554 Computer Vision: Course Project</span><br>
<span style="font-size: 18px; line-height: 1.5em;">Virginia Tech</span>
<hr>

<!-- Please see <a href="http://vision.cs.utexas.edu/projects/adapted_attributes/">this</a> for an example of how to lay out the various details of your project. You may need to provide more details than this, because you will not be submitting an associated paper to accompany the webpage. So the page should be self-contained. -->

<!-- Goal -->
<h2>Abstract</h2>
Automatic image coloration is a challenging problem in computer vision, motivated by its potential applications in image restoration, enhancement, and creative tasks. This project explores the performance of various deep learning architectures in transforming grayscale images into realistic color versions, aiming to understand the theoretical underpinnings and practical performance of these models. The approach involves experimenting with a progression of architectures: a simple CNN-based encoder-decoder, a U-Net with a pre-trained ResNet backbone, and a conditional GAN with the aforementioned U-Net as the generator, incorporating adversarial loss. Preliminary results indicate that as model complexity increases, the generated images exhibit improved color fidelity and realism, highlighting the potential of GANs in achieving context-aware and high-quality colorization. We observe an improvement in Mean Absolute Error (MAE) from 0.0848 for the CNN-based Encoder-Decoder to 0.0807 for cGAN. Similarly we also observe an improvement Learned Perceptual Image Patch Similarity (LPIPS) score (using VGG) from 0.148 for the CNN-based Encoder-Decoder to 0.132 for cGAN.   
<!-- Plans for experiments. Describe the experimental setup you will follow. Describe the datasets you
plan to use, what code you will implement yourself, what existing code you will borrow (if any), and
what you would define as a success for the project. If you intend to collect your own data, provide a
description of the procedure that you will follow. Provide a list of experiments that you will perform.
Describe what you expect the experiments to reveal, or what is uncertain about the potential
outcomes -->

<!-- <br><br> -->
<!-- figure -->
<!-- <h3>Teaser figure</h3> -->
<!-- A figure that conveys the main idea behind the project or the main application being addressed. -->
<!-- <br><br> -->
<!-- Main Illustrative Figure --> 
<!-- <div style="text-align: center;"> -->
<!-- <img style="height: 200px;" alt="" src="mainfig.png"> -->
<!-- </div> -->
<br><br><br>
<img src="Images/Diagram.png" alt="Trulli" class="custom-img"/>

<!-- <br><br> -->
<!-- Introduction -->
<!-- <h3>Introduction</h3>
Motivation behind the problem you are solving, what applications it has, any brief background on the particular domain you are working in (if not regular RBG photographs), etc. If you are using a new way to solve an existing problem, briefly mention and describe the existing approaches and tell us how your approach is new. -->

<!-- <br><br> -->
<!-- Approach -->
<h2>Introduction</h2>
Image colorization—the process of converting grayscale images into plausible color versions—has garnered significant interest due to its wide-ranging applications, including the restoration of historical photographs, enhancement of medical imaging, and the generation of visually enriched content for creative industries. This task is inherently challenging, as it requires a model to infer appropriate colors based on contextual and semantic information present in the grayscale input.

Traditional methods for image colorization often relied on manual intervention or heuristic algorithms, which were labor-intensive and lacked generalizability across diverse image datasets. The advent of deep learning has revolutionized this field, enabling the development of models that can learn complex mappings from grayscale to color images directly from data. Notable approaches include convolutional neural networks (CNNs) and generative adversarial networks (GANs), which have demonstrated remarkable success in producing realistic colorizations. For instance, Zhang et al. proposed a deep learning approach that combines a CNN with high-level features extracted from a pre-trained model, achieving impressive results in automatic colorization tasks [1]. Similarly, Deshpande et al. introduced a variational approach to produce diverse and contextually appropriate colorizations, highlighting the potential of probabilistic models in this domain [3].

Despite these advancements, challenges remain, particularly in generating contextually accurate and diverse colorizations. Recent research has focused on leveraging generative models to address these issues. Wu et al. introduced a method that utilizes a pretrained GAN to provide rich and diverse color priors, enabling the production of vivid and varied colorizations [2].

In this project, we aim to conduct a comparative study of various deep learning architectures for image colorization, including simple CNN-based encoder-decoders, U-Net architectures with pre-trained backbones, and GAN-based frameworks. By systematically evaluating these models across multiple datasets, we seek to identify the most effective architecture for producing realistic and high-quality colorizations, thereby advancing the current state of the art in automatic image colorization.




<section id="approach">
  <h2>Approach</h2>
  To solve the problem of automatic image coloration, we employed a progressive approach involving multiple architectures. This approach was designed to analyze and compare the effectiveness of various models in transforming grayscale images into realistic colorized versions. The process included data preprocessing and three main modeling phases: baseline CNN encoder-decoder, U-Net architecture with a ResNet-18 backbone, and a Conditional GAN (cGAN) with a PatchGAN discriminator [4].

  <h3>Data Preprocessing</h3>
  <ul>
    <li>The images from the COCO dataset were converted into the Lab color space, where the L* channel (lightness) represents grayscale information, and the A* and B* channels encode color information.</li>
    <li>The L* channel was used as the input to the model, while the A* and B* channels served as the ground truth outputs for supervised learning.</li>
    <li>To ensure consistency, all images were resized to 256x256 pixels and normalized to a range between -1 and 1 for both input L channel and output AB channel.</li>
    <li>Random horizontal flipping was applied to the images before breakdown into L and AB channel to enhance model generalization and robustness.</li>
  </ul>
  <br>
  <img src="Images/LAB.png" alt="Trulli" class="custom-img"/>
<!-- add the three architecture diagram form borrowing or draw them on your own or tell tushar to put them   -->
  <h3>Model Architectures</h3>
  <ul>
    <li>
      <strong>CNN Encoder-Decoder:</strong> We started with a simple encoder-decoder CNN architecture as our baseline. The encoder extracted latent grayscale features, while the decoder reconstructed the A* and B* color channels. This model served as a foundational benchmark for further comparisons.
      <ul>
        <li>
          <strong>Encoder:</strong> This module captures the spatial features by applying multiple convolution layers followed by downsampling like max pooling. This helps in reducing the dimensions and extracts important features.</li>
          <li><strong>Lantent Representation:</strong> The encoder outputs a feature representation which is compact in nature and gives the most vital spatial and semantic information
          </li><li><strong>Decoder:</strong> The decoder takes the input from the encoder and performs upsampling with operations like transposed convolutions or interpolation. It thereby increases the spatial dimensions, performs convolutions to refine the features to output the finaloutput.
          </li><li><strong>Output layer:</strong> We have a final convolution layer that outputs the desired output which is in the form of a RGB image.</strong>
        </li>
      </ul>
    </li>

    <br>
    <li>
      <strong>U-Net with ResNet-18 Backbone:</strong> The U-Net architecture, with a ResNet-18 backbone in the encoder, was the second phase of our approach. Pre-trained weights from ImageNet were loaded into the encoder to leverage transfer learning, enhancing the model's ability to extract meaningful features. The pre-trained U-Net was fine-tuned on our dataset for 20 epochs to adapt to the specific task of image colorization.
      <ul>
        <li>
          <strong>Encoder: </strong> Similar to the Encoder-Decoder module, the Encoder in UNet captures importatant spatial features, reducing the image size while extracting important features. Here we replace the traditional encoder with a ResNet-18 architecture. ResNet-18 consists of convolutional layers and skip connections (in the form of residual blocks) that help to learn deeper representations without having the vanishing gradient problem.  
        </li><li><strong>Connection: </strong> The deepest part of the UNet from the encoder gets connected to the decoder, which has captured the most important spatial features
        </li><li><strong>Decoer: </strong> This model performs upsamling and recaptures the image resolution by concantenating them with corresponding feature maps from encoder.
      </li><strong>Skip Connections: </strong> The skips connections link the encoder layer to the corresponding decoder layers, which preserves fine-grained spatial features which get lost during the downsampling operation. 
    </li><strong>Training: </strong> We used predefined weights from ImageNet using the Fast AI library. THe UNet was then trained using the Adam optimizer for 20 epochs with L1 loss as the criterion, since it - preserves spatial structure, it is robust to any outliers and has a better gradient flow. 
    </li>
      </ul>
    </li>

    <center><img src="Images/UNet Arch.png" alt="UNet Architecture" class="custom-img"></center>
    
    <br>
    <li>
      <strong>DCGAN:</strong> The pre-trained U-Net was integrated as the generator in a Deep Convolutional GAN (DCGAN) framework. DCGANs were chosen because they condition the output (colorized image) on the input (grayscale image), enabling the generator to produce outputs that are contextually consistent with the input image.
      <ul>
        <li><strong>PatchGAN Discriminator:</strong> We used a PatchGAN discriminator, which evaluates the realism of small image patches rather than the entire image. This approach ensures local coherence, as the discriminator classifies each patch of the generated image as real or fake. PatchGAN was chosen because it is effective in preserving fine details and textures, which are critical for realistic image colorization.</li>
        <li><strong>Generator:</strong> The DCGAN uses the UNet module as it's generator output. It creates images using the UNet module by effectively capturing spatial dimensions with skip layers. This enables accurate mappings from grayscale to colorized outputs.
        </li><li> <strong>Discriminator: </strong> This is a custom CNN module which differentiates between the real images or generated ones. This assesses the authenticity, which makes use of convolutions to differentiate between real or generated real and generated colorizations.
        </li><li><strong>Training: </strong> We trained the DCGAN using BCE Loss for 10 epochs. We used the adam optimizer again for training the discriminator with decay rate taken as 0.9.</li> 
      </ul>
    </li>

    <center><img src="Images/GAN Arch.png" alt="GAN Architecture" class="custom-img"></center>


  </ul>

  <h3>Implementation Details</h3>
  <ul>
    <li><strong>Dataset:</strong> The COCO dataset was used, with a subset of 10,000 images. The dataset was split into 8,000 images for training and 2,000 for testing.</li>
    <li><strong>Training Setup:</strong> The CNN encoder-decoder was trained for 100 epochs as a baseline. The U-Net architecture was pre-trained for 20 epochs with ImageNet weights before being fine-tuned. The Conditional GAN was trained by integrating the U-Net generator with the PatchGAN discriminator.</li>
    <li><strong>Pre-trained Models:</strong> The ImageNet pre-trained weights for the ResNet-18 backbone were utilized to expedite convergence and improve feature extraction. The pre-trained architectures were obtained from publicly available PyTorch repositories.</li>
  </ul>

  <h3>Challenges and Solutions</h3>
  <ul>
    <li><strong>Challenge:</strong> Balancing model complexity with computational constraints.
      <ul>
        <li><strong>Solution:</strong> Transfer learning using ImageNet pre-trained weights allowed us to start training from a strong initialization, reducing training time.</li>
      </ul>
    </li>
    <li><strong>Challenge:</strong> Fine-tuning PatchGAN to work effectively for image colorization.
      <ul>
        <li><strong>Solution:</strong> Extensive experimentation with the discriminator's patch size to ensure optimal detection of local inconsistencies.</li>
      </ul>
    </li>
  </ul>

  <h3>Design Choices</h3>
  <ul>
    <li><strong>Why Conditional GAN:</strong> cGANs condition the output on the input, ensuring that the generated image aligns with the grayscale input's semantic content. This made them particularly suitable for the image coloration task, as they could handle context-sensitive colorization.</li>
    <li><strong>Why PatchGAN Discriminator:</strong> PatchGAN discriminates at the level of image patches, which is effective for ensuring local coherence in generated images. This is particularly useful for image colorization, where local color consistency is critical.</li>
  </ul>

</section>

<section id="experiments-results">
  <h2>Experiments and Results</h2>

  <h3>Experimental Setup</h3>
  <ul>
    <li><strong>Datasets:</strong> A subset of the COCO dataset with 10,000 images. The training set consisted of 8,000 images, and the test set included 2,000 images.</li>
    <li><strong>Metrics:</strong> L1 Error was used to evaluate pixel-wise accuracy during training. Mean Absolute Error (MAE) and Learned Perceptual Image Patch Similarity (LPIPS) score (using latent features from pretrained VGG) [7] are used to quantitatively compare the model performances.</li>
  </ul>

  <h3>Results</h3>
  <ul>
    <li><strong>CNN Encoder-Decoder:</strong> Achieved baseline results with limited ability to capture complex features, showing noticeable artifacts in the colorized outputs. We got a validation loss of 0.0836 after training it for 10 epochs. We trained the module using the L1 loss criterion with learning rate set as 0.001.</li>
    <li><strong>U-Net with ResNet-18 Backbone:</strong> Improved performance compared to the baseline, with enhanced feature extraction and better contextual colorization. The UNet was trained for 20 epochs and acheieved a L1 loss of 0.081 . We set the learning rate to 0.0001</li>
    <li><strong>Conditional GAN:</strong> Produced the most realistic and visually consistent outputs. The incorporation of adversarial loss with the PatchGAN discriminator significantly reduced artifacts and improved local color coherence. We achieved a discriminator loss of 0.1067 and discriminator loss of 4.5 after training for 10 epochs. We also tried regularization using Wasserstein Loss that was used by the Discriminator to evaluate whether the generated image is real or fake. We got a greater accuracy with both the Discriminator and Generator loss converging with losses of around 0.003.</li>
    <img src="Images/GAN Arch.png" alt="Trulli" class="custom-img"/>
    <img src="Images/UNet Arch.png" alt="Trulli" class="custom-img"/>
  
    The MAE and LPIPS value on the test dataset for each model is listed in Table 1.

    <h5>Table 1: Results</h5>
    <table border="1" style="border-collapse: collapse; width: 50%;">
      <thead>
          <tr>
              <th>Model</th>
              <th>MAE</th>
              <th>LPIPS</th>
          </tr>
      </thead>
      <tbody>
          <tr>
              <td>CNN Encoder-Decoder</td>
              <td>0.0848</td>
              <td>0.148</td>
          </tr>
          <tr>
              <td>U-Net (ResNet-18 backbone)</td>
              <td>0.0846</td>
              <td>0.141</td>
          </tr>
          <tr>
            <td>cGAN</td>
            <td>0.0807</td>
            <td>0.132</td>
        </tr>
      </tbody>
  </table>
  </ul>

  Outputs by each model on 5 sample images from the test dataset are listed below:

  <img src="Images/result_1.png" alt="Trulli" class="custom-img"/>
  <img src="Images/result_2.png" alt="Trulli" class="custom-img"/>
  <img src="Images/result_3.png" alt="Trulli" class="custom-img"/>
  <img src="Images/result_4.png" alt="Trulli" class="custom-img"/>
  <img src="Images/result_5.png" alt="Trulli" class="custom-img"/>

  <h3>Parameter Analysis</h3>
    Adjusting the patch size in the PatchGAN discriminator affected the realism of local features.
    Larger patches reduced sensitivity to finer details, while smaller patches improved local
    consistency at the expense of global structure.
    <ul>
      <li><strong>Epoch: </strong> We have tried to use the number of epochs for getting better accuracy but also making sure we do not overfit any model. Hence for training the UNet we used 20 epochs, after which we were seeing results getting converged with no frther reductions in losses. Similarly for DCGAN, we kept the epoch numbers to 10 so that the generator does not overflow and discriminator does not overfit.</li>
      <li><strong>Learning Rate: </strong> Since learning rate is a vital parameter in training neural networks since they control by how much the weights should be adjusted, we kept it as 0.01 so as to avoid any irregularities and unstable updates that would have destabilized the training.</li>
      <li><strong>Beta1: </strong> Beta1 used in training the GAN is also a important parameter for the ADAM optimizer to take into account past weight updates for the current weights. We kept it as 0.5 to prevent any rapid updates, which allowed for stable training especially in earlier stages.</li>
      <li><strong>Criterion: </strong> We used L1 loss as the criterion for training because it allows for pixelwise accuracy which hlps in generating realistic images. We used the BCE loss for training the GAN which is a common method, the discriminator learns to differentiate between real and fake images and generator is given the taks to fool the discriminator into classfying the generated images as real ones. </li>
      <li><strong>Batch Size: </strong> The batch size allows the neural network to update the weights in batches (controls the frequency as to when weights are updated). Since larger batch size required more memory and smaller ones lead to noiser gradients, we set the batch size as 32. </li>
      <li><strong>Noise Dimension: </strong> The noise dimension provides the size of random noise fed to the generator, larger ones lead to overfitting but help generate more creative and realistic images. We kept it as 100, which is usually used in GANs to provide more diversity.  </li>
    </ul>

  <h3>Trends</h3>
    The progression from a simple encoder-decoder to the U-Net and GAN demonstrated a clear
    improvement in colorization quality. This trend aligns with the increasing complexity and capability
    of the models to capture semantic information.
    The results validated the effectiveness of cGANs with PatchGAN discriminators for image
    colorization, offering a significant improvement over baseline models. These findings highlight
    the importance of balancing local and global features in generating realistic colorizations.
</section>

<br>
<!-- Results -->
<section id="conclusion">
  <h2>Conclusion</h2>
    This report has described the design, implementation, and evaluation of various deep learning
    architectures for automatic image coloration. Starting with a baseline CNN encoder-decoder, 
    progressing to a U-Net with a ResNet-18 backbone, and culminating with a Conditional GAN 
    incorporating a PatchGAN discriminator, the study highlighted the strengths and limitations of 
    each approach. The results demonstrated that increasing model complexity, along with the 
    integration of adversarial loss, significantly improved the realism and coherence of the colorized outputs.
    The U-Net architecture with pre-trained ResNet-18 weights provided a strong foundation for 
    feature extraction and contextual understanding, while the Conditional GAN produced the 
    most visually realistic results by leveraging adversarial training and patch-level discrimination. 
    These findings validated the effectiveness of combining local and global feature learning to achieve 
    high-quality colorizations.
  <h4>Future Improvements</h4>
  To further enhance the approach, several improvements could be considered:
  <ul>
    <li><strong>Data Augmentation:</strong> Expanding the dataset with more diverse images and applying advanced augmentation techniques could help the model generalize better to unseen images.</li>
    <li><strong>Loss Functions:</strong> Experimenting with perceptual loss or combining MSE with adversarial loss might improve the model’s ability to generate outputs that are both quantitatively and qualitatively superior.</li>
    <li><strong>Higher-Resolution Models:</strong> Incorporating super-resolution techniques or adapting the model for higher-resolution inputs could enhance the fine details in the colorized images.</li>
  </ul>
    Overall, this study serves as a foundation for understanding and improving automatic image 
    coloration, paving the way for more robust and application-specific advancements in the field.
</section>
<section id='to-do'>
<li>
  add three model architeures that we used unet and GAN-Tushar
</li>
<li>
If you utilized code that you did not write for part of your project, clearly describe where you obtained that code.
</li>
<li>
Describe what obstacles you faced and how you addressed them. 
</li>
<li>
Justify any design choices or judgment calls you made in your approach.
</li>
<li>
mETRICS TO EVALUATE ON, 1 GRAPH About the training error for the three model, 1 table and qualitative results 3 success two failures.
</li>
<li>
Add more references.
</li>
<li>
Maybe can we add a little more content in paraeter analysis?
</li>
</section>

<h2>References</h2>
<ol>
<li>
R. Zhang, P. Isola, and A. A. Efros, "Colorful Image Colorization," Proceedings of the European Conference on Computer Vision (ECCV), pp. 649–666, 2016. [Online]. Available: https://arxiv.org/abs/1603.08511
</li>
<li>
Y. Wu, P. Zhang, and C. Zhang, "Generative Colorization for Diverse Images," arXiv preprint arXiv:2108.08826, 2021. [Online]. Available: https://arxiv.org/abs/2108.08826
</li>
<li>
A. Deshpande, J. Lu, M. Yeh, M. Jin, and D. Forsyth, "Learning Diverse Image Colorization," Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6837–6845, 2017. [Online]. Available: https://arxiv.org/abs/1612.01958
</li>
<li>
Isola, Phillip, et al. "Image-to-image translation with conditional adversarial networks." Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.
</li>
<li>
C. Li and M. Wand. Precomputed real-time texture synthesis with markovian generative adversarial networks. ECCV, 2016
</li>
<li>
O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image segmentation. In MICCAI, 2015
</li>
<li>
Zhang, R., Isola, P., Efros, A. A., Shechtman, E., and Wang,
  O. The unreasonable effectiveness of deep features as a
  perceptual metric. In Proceedings of the IEEE conference
  on computer vision and pattern recognition, 2018.
</li>
</ol>
<!-- <br><br> -->

<!-- Main Results Figure --> 
<!-- <div style="text-align: center;">

</div>
<br><br> -->

<!-- Results -->
<!-- <h3>Qualitative results</h3>
Show several visual examples of inputs/outputs of your system (success cases and failures) that help us better understand your approach. -->

<!-- Main Results Figure --> 
<!-- <div style="text-align: center;">
<img style="height: 300px;" alt="" src="qual_results.png">
</div>
<br><br> -->

<!-- Conclusion -->
<!-- <h3>Conclusion</h3>
This report has described .... Briefly summarize what you have done. 
<br><br> -->

<!-- References -->
<!-- <h3>References</h3>
Provide a list of references to other work that supported your project.
<br><br> -->


  <hr>
  <footer> 
  <p>© Gauransh Sawhney, Tushar Deshpande and Daksh Dave </p>
  </footer>
</div>
</div>

<br><br>

</body></html>